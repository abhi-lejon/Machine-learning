{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP35t9MAlCK6EujSXQDvCwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi-lejon/Machine-learning/blob/master/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUBgXQ5ZAbOx"
      },
      "source": [
        "#As of now we are using pandas for data analysis.\n",
        "#But for large complicated dataset we need to go for some bigdata technologies like Hadoop, Pyspark etc...\n",
        "#Let we start learning pyspark."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQB1LKqpAd_J"
      },
      "source": [
        "#Generally RAM will have size of 4GB,6GB or 8GB... If our data size is less than 8GB then it works fine, if the size of data greater than 8GB will it works?\n",
        "#System collapes.\n",
        "#So we need something like clustered env, distributed or parallel systems. Here the data get distributed into different systems and we can do operations easily."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK6XvLuKAx7W"
      },
      "source": [
        "#Bigdata - We call any data as big data\n",
        "#1 Volume - Size of the data\n",
        "#2 Velocity - Speed at which the data gets generated.\n",
        "#3 Variety - Different types of data.\n",
        "#4 Veracity - Trusting the data interms of accuracy. #Quality, uniqueness, randomness.\n",
        "#5 Value - Using of bigdata is not of use unless we can turn it into value."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affKjr2GAx-I"
      },
      "source": [
        "#Hadoop\n",
        "#Storage Unit- HDFS- It is a fie system which hadoop stores the data. By partitioning the data it will provide the data to different applications running on the clusters.\n",
        "#It divides tha data and stores it in different parts. (Part 1, Part 2...)\n",
        "#Map Reduce-Compute engine- It will work on the small parts of data, Reduce gathers the result set from different parts.\n",
        "#YARN - Resource manager - Master node and Slave nodes The request comes to Master node and it will direct the task to corresponding slave node."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4UyrlmCGe9Y"
      },
      "source": [
        "#This is about Hadoop, Lets now look into Apache.\n",
        "In Big data Spark fits in MapReduce region. Mapredce is replaced by Spark.\n",
        "HDFS is by hadoop. If we dont want to use HDFS we can use local data like csv file etc.. We can connect to S3.\n",
        "In terms of YARN we can use spark internal resource manager, Kubernetes, MESOS.\n",
        "If we are using HDFS and YARN the we call it as Distributed system, If nto we call it as Standalone system.\n",
        "So spark can be deployed in two moods, Distributed system and Standalone system."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaP8nlwiGfAN"
      },
      "source": [
        "# Source>Ingest>Process>Store>Serve\n",
        "#Hadoop is only for batch processing of data, for stream data we need to use kafka, and it supports only Java, for scheduling we need to use other things.\n",
        "#But Spark as everything inbuilt. SQL,ML libraries, real time data processing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1X3O6MSGfDN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxuldhAWGfGK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlY4QfGFAyBE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6moBlvAyDx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmztyquaAyGu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpEkNMYjAeBi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO3lB7JrAeEa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}